batch and momentum

batch:  一个一个“包”，

shuffle：在每一个epoch里batch都被重新打乱

为什么要用batch

batch and no batch：

大招和连续平a：技能cd的长短，有利有弊<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231214191050019.png" alt="image-20231214191050019" style="zoom:67%;" />

GPU具有平行运算的能力，所以batch适当加大，减小epoch反而能faster

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231214191636311.png" alt="image-20231214191636311" style="zoom:67%;" />

大的batch size具有速度上的优势，却在准确度方面出现了问题

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231214191942094.png" alt="image-20231214191942094" style="zoom:67%;" />

这是因为更小的batch更加的“noisy”

较小的batch有助于增加模型的泛化能力。这是因为每次使用小批量数据进行训练，模型都能从每个batch中学到一些新信息。较小的batch通常能提供更噪声的梯度估计，这种噪声有时候可以帮助模型跳出局部最小值，从而找到更好的全局最小值。

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231214192509364.png" alt="image-20231214192509364" style="zoom:67%;" />

就算通过调整big batch's learning rate and let the training accuracy similar to the small one,the testing accuracy is still bad which means overfitting

![image-20231214193708406](C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231214193708406.png)

how to have both fish and bear paws(optional)

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231214193802505.png" alt="image-20231214193802505" style="zoom:67%;" />

momentum：对抗鞍部和local mininum

实际上就是动量![image-20231214194020081](C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231214194020081.png)

现在的gradient加上过去所以gradient的总和：结合过去的经验，顺应趋势

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231214194500084.png" alt="image-20231214194500084" style="zoom:67%;" />

也可以理解为是一种矢量的相加

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231214194536146.png" alt="image-20231214194536146" style="zoom:67%;" />

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231214194711618.png" alt="image-20231214194711618" style="zoom:67%;" />