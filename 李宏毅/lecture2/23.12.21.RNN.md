Slot filling

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217142957430.png" alt="image-20231217142957430" style="zoom:67%;" />

输入词汇计算机无法理解，必须转化成vector形式

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217144128391.png" alt="image-20231217144128391" style="zoom:67%;" />

让机器变得“有记忆”：RNN

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217145605002.png" alt="image-20231217145605002" style="zoom:67%;" />

一个RNN的简单例子：他多了一个memory，共同参与。它的前后顺序是不可以改变的，改变了会影响输出

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217151217848.png" alt="image-20231217151217848" style="zoom:67%;" />

这里运用RNN成功解决了对整体词义不理解的问题

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217151512676.png" alt="image-20231217151512676" style="zoom:67%;" />

不同类型的RNN

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217151757537.png" alt="image-20231217151757537" style="zoom:67%;" />

双向的RNN，相当于完整的看完了整个句子，而不是只看一半

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217152423088.png" alt="image-20231217152423088" style="zoom:67%;" />

这三个门的开关都是由机器自己学习的 

这其中long的含义：一般的memory cell每次在有新的input进来之后都会更新，但LSTM就有可能保留更久以前的input

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217172436222.png" alt="image-20231217172436222" style="zoom:67%;" />

具体解析：

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217173054173.png" alt="image-20231217173054173" style="zoom:67%;" />

非常生动形象的演示了具体演示过程：4input，1output

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217200325191.png" alt="image-20231217200325191" style="zoom:67%;" />

RNN也是通过gradient decent train实现的

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217203402822.png" alt="image-20231217203402822" style="zoom:67%;" />

RNN导致的问题：gradient vanishing：是由sigmoid导致的？换成ReLU?

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217222532702.png" alt="image-20231217222532702" style="zoom:67%;" />

真正的原因：他有time sequence，同一个weight在不同的时间点被反复的使用

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217223034028.png" alt="image-20231217223034028" style="zoom:67%;" />

因此，用LSTM或者RNN会更好

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217230044788.png" alt="image-20231217230044788" style="zoom:67%;" />

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217230122738.png" alt="image-20231217230122738" style="zoom:67%;" />

具体应用.RNN可以做到更多

给机器看很多文章判断情感趋向

输入是一个character sequence

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217230719787.png" alt="image-20231217230719787" style="zoom:67%;" />

predict 关键词汇

输入一个有标记重点的document让机器自己学习

这里是多对一<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231217230923437.png" alt="image-20231217230923437" style="zoom:67%;" />

多对多

但是输入长输出短，例如语音识别

trimming：简单的去处重复，不太可取

![image-20231220170442886](C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220170442886.png)

如何解决叠字问题？<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220170656146.png" alt="image-20231220170656146" style="zoom:67%;" />

原来是预先就设好label了，然后再机器学习

![image-20231220170952241](C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220170952241.png)

还有空白哎，Google就用CTC，让机器自己学习，就算在data里从来没有这个词汇他也能识别出来，how？不需要label？

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220171148825.png" alt="image-20231220171148825" style="zoom:67%;" />

 no limitation，机器翻译

好神奇，就感受一下即可

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220171629218.png" alt="image-20231220171629218" style="zoom:67%;" />

加一个断，如何做？再说吧

听说结果还不错

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220171905501.png" alt="image-20231220171905501" style="zoom:67%;" />

更妙了，直接从语音到文字，省去了语言识别打label的过程<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220172229215.png" alt="image-20231220172229215" style="zoom:67%;" />

再次强调了LSTM是具有记忆力的：LSTM 它有记忆力，他不会忘记加上右括号

在语法解析的运用，这是一个树状的结构他实际上超出了sequence，但是也能处理

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220214749018.png" alt="image-20231220214749018" style="zoom:67%;" />

如何实现语音直接转文字，先在数据集上训练，实时声音数据转化为vector之后再与训练好的vector进行比较

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220220952265.png" alt="image-20231220220952265" style="zoom:67%;" />

具体该如何实现：1.先encoder：把声音转化为向量

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220221131678.png" alt="image-20231220221131678" style="zoom:67%;" />

还要有几个decoder

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220221514448.png" alt="image-20231220221514448" style="zoom:67%;" />

机器如何做到人脑的思考方式？

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220222448886.png" alt="image-20231220222448886" style="zoom:67%;" />

运用：构建了一个真正智能的专家系统，真正以人类的方式思考

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220222928219.png" alt="image-20231220222928219" style="zoom:67%;" />

泰裤辣

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220223228720.png" alt="image-20231220223228720" style="zoom:67%;" />

这位更是重量级：薄纱托福，先语言识别，再学习，最后再问他问题<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220223354734.png" alt="image-20231220223354734" style="zoom:67%;" />

模型结构，这像不像chatgpt？！

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220223551378.png" alt="image-20231220223551378" style="zoom:67%;" />

不同模型的融合往往效果更好

<img src="C:\Users\叶xz\AppData\Roaming\Typora\typora-user-images\image-20231220224716662.png" alt="image-20231220224716662" style="zoom:67%;" />